__all__ = ["FFNNTransducerTrainOptions", "FFNNTransducerPrunedTrainOptions", "train", "train_pruned"]

from dataclasses import dataclass
from typing import Tuple

import torch
from ..common.pytorch_modules import lengths_to_padding_mask
from i6_core.returnn.training import ReturnnTrainingJob
from minireturnn.torch.context import RunCtx

from i6_experiments.common.setups.serialization import PartialImport

from ..common.serializers import get_model_serializers
from ..common.train import TrainOptions
from ..common.train import train as train_
from .pytorch_modules import FFNNTransducerConfig, FFNNTransducerModel


# Adapted from fast_rnnt but doesn't require same dimension of am and lm
def _do_rnnt_pruning(am: torch.Tensor, lm: torch.Tensor, ranges: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """Prune the output of encoder(am) and prediction network(lm) with ranges
    generated by `get_rnnt_prune_ranges`.

    Args:
      am:
        The encoder output, with shape (B, T, E)
      lm:
        The prediction network output, with shape (B, S + 1, P)
      ranges:
        A tensor containing the symbol indexes for each frame that we want to
        keep. Its shape is (B, T, s_range), see the fast_rnnt docs in
        `get_rnnt_prune_ranges` for more details of this tensor.

    Returns:
      Return the pruned am and lm with shape (B, T, s_range, E) and (B, T, s_range, P)
    """
    # am (B, T, E)
    # lm (B, S + 1, P)
    # ranges (B, T, s_range)
    assert ranges.shape[0] == am.shape[0], (ranges.shape[0], am.shape[0])  # B
    assert ranges.shape[0] == lm.shape[0], (ranges.shape[0], lm.shape[0])  # B
    assert am.shape[1] == ranges.shape[1], (am.shape[1], ranges.shape[1])  # T
    (B, T, s_range) = ranges.shape
    (_, _, E) = am.shape
    (_, _, P) = lm.shape

    # (B, T, s_range, E)
    am_pruning = am.unsqueeze(2).expand((B, T, s_range, E))

    # (B, T, s_range, P)
    lm_pruning = torch.gather(
        lm,
        dim=1,
        index=ranges.reshape(B, T * s_range, 1).expand((B, T * s_range, P)),
    ).reshape(B, T, s_range, P)
    return am_pruning, lm_pruning


@dataclass
class FFNNTransducerTrainOptions(TrainOptions):
    enc_loss_scale: float
    pred_loss_scale: float


@dataclass
class FFNNTransducerPrunedTrainOptions(FFNNTransducerTrainOptions):
    delay_penalty: float
    skip_epochs_before_pruned_loss: int
    prune_range: int
    smoothed_loss_scale: float


def _train_step(
    *,
    model: FFNNTransducerModel,
    data: dict,
    run_ctx: RunCtx,
    enc_loss_scale: float,
    pred_loss_scale: float,
    **_,
):
    from i6_native_ops.monotonic_rnnt import monotonic_rnnt_loss

    audio_samples = data["data"]  # [B, T, 1]
    audio_samples_size = data["data:size1"].to(device=audio_samples.device)  # [B]

    targets = data["classes"].long()  # [B, S]
    targets_size = data["classes:size1"]  # [B]
    targets_size = targets_size.to(device=audio_samples.device)

    loss_norm_factor = torch.sum(targets_size)

    encoder_states, encoder_states_size = model.forward_encoder(
        audio_samples=audio_samples, audio_samples_size=audio_samples_size
    )  # [B, T, E], [B]
    encoder_logits = model.encoder_output.forward(encoder_states)  # [B, T, V]

    pred_states = model.forward_prediction_network(targets=targets)  # [B, S+1, P]
    pred_logits = model.prediction_output.forward(pred_states)

    logits = model.forward_joint_network(
        encoder_states=encoder_states,
        encoder_states_size=encoder_states_size,
        pred_states=pred_states,
        targets_size=targets_size,
    )

    has_mismatch = False
    for b in range(encoder_states_size.size(0)):
        if targets_size[b] > encoder_states_size[b]:
            print(
                data["seq_tag"][b], "has", targets_size[b], "tagets but only", encoder_states_size[b], "encoder states"
            )
            has_mismatch = True

    if not has_mismatch:
        rnnt_loss = monotonic_rnnt_loss(
            acts=logits.to(dtype=torch.float32),
            labels=targets,
            input_lengths=encoder_states_size,
            label_lengths=targets_size,
            blank_label=model.target_size - 1,
        ).sum()
    else:
        rnnt_loss = torch.zeros([], dtype=torch.float32, device=logits.device)

    run_ctx.mark_as_loss(name="mono_rnnt", loss=rnnt_loss, inv_norm_factor=loss_norm_factor)

    if enc_loss_scale != 0:
        ctc_log_probs = torch.nn.functional.log_softmax(encoder_logits, dim=-1).transpose(0, 1)  # [T, B, V]
        ctc_loss = torch.nn.functional.ctc_loss(
            log_probs=ctc_log_probs,
            targets=targets,
            input_lengths=encoder_states_size,
            target_lengths=targets_size,
            blank=model.target_size - 1,
            reduction="sum",
            zero_infinity=True,
        )
        run_ctx.mark_as_loss(name="enc_ctc", loss=ctc_loss, inv_norm_factor=loss_norm_factor, scale=enc_loss_scale)

    if pred_loss_scale != 0:
        pred_logits = pred_logits[:, :-1, :].transpose(1, 2)  # [B, V, S]
        ce_loss = torch.nn.functional.cross_entropy(
            pred_logits,
            targets,
            reduction="none",
        )
        seq_mask = lengths_to_padding_mask(targets_size)
        ce_loss = (ce_loss * seq_mask).sum()
        run_ctx.mark_as_loss(name="pred_ce", loss=ce_loss, inv_norm_factor=loss_norm_factor, scale=pred_loss_scale)


def _train_step_pruned(
    *,
    model: FFNNTransducerModel,
    data: dict,
    run_ctx: RunCtx,
    enc_loss_scale: float,
    pred_loss_scale: float,
    delay_penalty: float,
    skip_epochs_before_pruned_loss: int,
    prune_range: int,
    smoothed_loss_scale: float,
    **_,
):
    import fast_rnnt

    audio_samples = data["data"]  # [B, T, 1]
    audio_samples_size = data["data:size1"].to(device=audio_samples.device)  # [B]

    targets = data["classes"].long()  # [B, S]
    targets_size = data["classes:size1"]  # [B]
    targets_size = targets_size.to(device=audio_samples.device)

    encoder_states, encoder_states_size = model.forward_encoder(
        audio_samples=audio_samples, audio_samples_size=audio_samples_size
    )  # [B, T, E], [B]
    encoder_logits = model.encoder_output.forward(encoder_states)  # [B, T, V]

    pred_states = model.forward_prediction_network(targets=targets)  # [B, S+1, P]
    pred_logits = model.prediction_output.forward(pred_states)

    has_mismatch = False
    for b in range(encoder_states_size.size(0)):
        if targets_size[b] > encoder_states_size[b]:
            print(
                data["seq_tag"][b], "has", targets_size[b], "tagets but only", encoder_states_size[b], "encoder states"
            )
            has_mismatch = True

    if not has_mismatch:
        begin = torch.zeros_like(targets_size)
        boundary = torch.stack([begin, begin, targets_size, encoder_states_size], dim=1)

        smoothed_loss, (px_grad, py_grad) = fast_rnnt.rnnt_loss_smoothed(
            lm=pred_logits,
            am=encoder_logits,
            symbols=targets,
            termination_symbol=model.target_size - 1,
            lm_only_scale=pred_loss_scale,
            am_only_scale=enc_loss_scale,
            boundary=boundary,
            reduction="sum",
            rnnt_type="modified",  # mono-rnnt
            delay_penalty=delay_penalty,
            return_grad=True,
        )

        if run_ctx.epoch > skip_epochs_before_pruned_loss:
            ranges = fast_rnnt.get_rnnt_prune_ranges(
                px_grad=px_grad,
                py_grad=py_grad,
                boundary=boundary,
                s_range=prune_range,
            )  # [B, T, prune_range]

            am_pruned, lm_pruned = _do_rnnt_pruning(
                am=encoder_states, lm=pred_states, ranges=ranges
            )  # [B, T, prune_range, E], [B, T, prune_range, P]

            joint_input = torch.concat([am_pruned, lm_pruned], dim=-1)  # [B, T, prune_range, E+P]
            logits = model.joint_net.forward(joint_input)  # [B, T, prune_range, V]

            pruned_loss = fast_rnnt.rnnt_loss_pruned(
                logits=logits,
                symbols=targets,
                ranges=ranges,
                termination_symbol=model.target_size - 1,
                boundary=boundary,
                rnnt_type="modified",
                delay_penalty=delay_penalty,
                reduction="sum",
                return_grad=False,
            )
        else:
            pruned_loss = torch.zeros([], dtype=torch.float32, device=encoder_logits.device)
    else:
        smoothed_loss = torch.zeros([], dtype=torch.float32, device=encoder_logits.device)
        pruned_loss = torch.zeros([], dtype=torch.float32, device=encoder_logits.device)

    loss_norm_factor = torch.sum(targets_size)

    run_ctx.mark_as_loss(
        name="smoothed_rnnt", loss=smoothed_loss, inv_norm_factor=loss_norm_factor, scale=smoothed_loss_scale
    )
    run_ctx.mark_as_loss(name="pruned_rnnt", loss=pruned_loss, inv_norm_factor=loss_norm_factor)


def train(
    options: FFNNTransducerTrainOptions,
    model_config: FFNNTransducerConfig,
) -> ReturnnTrainingJob:
    model_serializers = get_model_serializers(model_class=FFNNTransducerModel, model_config=model_config)
    train_step_import = PartialImport(
        code_object_path=f"{_train_step.__module__}.{_train_step.__name__}",
        hashed_arguments={
            "enc_loss_scale": options.enc_loss_scale,
            "pred_loss_scale": options.pred_loss_scale,
        },
        unhashed_arguments={},
        unhashed_package_root="",
        import_as="train_step",
    )

    return train_(options=options, model_serializers=model_serializers, train_step_import=train_step_import)


def train_pruned(
    options: FFNNTransducerPrunedTrainOptions,
    model_config: FFNNTransducerConfig,
) -> ReturnnTrainingJob:
    model_serializers = get_model_serializers(model_class=FFNNTransducerModel, model_config=model_config)
    train_step_import = PartialImport(
        code_object_path=f"{_train_step_pruned.__module__}.{_train_step_pruned.__name__}",
        hashed_arguments={
            "enc_loss_scale": options.enc_loss_scale,
            "pred_loss_scale": options.pred_loss_scale,
            "delay_penalty": options.delay_penalty,
            "skip_epochs_before_pruned_loss": options.skip_epochs_before_pruned_loss,
            "prune_range": options.prune_range,
            "smoothed_loss_scale": options.smoothed_loss_scale,
        },
        unhashed_arguments={},
        unhashed_package_root="",
        import_as="train_step",
    )

    return train_(options=options, model_serializers=model_serializers, train_step_import=train_step_import)
