{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/u/lukas.rilling/dev/\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "%matplotlib widget\n",
    "\n",
    "from returnn_training_progress import get_epoch_data\n",
    "from returnn_training_plot_nb import plot_df\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/tts_pretrained/no_specaug/asr_target_size/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf', '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/basic_init/no_specaug/asr_target_size/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf', '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/basic_init/no_specaug/tts_target_size/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf', '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/tts_pretrained/no_specaug/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf', '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/basic_init/no_specaug/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/tts_pretrained/no_specaug/asr_target_size/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf': '/tts_pretrained/no_specaug/asr_target_size/',\n",
       "  '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/basic_init/no_specaug/asr_target_size/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf': '/basic_init/no_specaug/asr_target_size/',\n",
       "  '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/basic_init/no_specaug/tts_target_size/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf': '/basic_init/no_specaug/tts_target_size/',\n",
       "  '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/tts_pretrained/no_specaug/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf': '/tts_pretrained/no_specaug/',\n",
       "  '/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/basic_init/no_specaug/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf': '/basic_init/no_specaug/'},\n",
       " 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globs = [\n",
    "    \"/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/*/no_specaug/*/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf\",\n",
    "    \"/u/lukas.rilling/experiments/glow_tts_asr_v2/alias/experiments/librispeech/joint_training/given_alignments/raw_audio/joint_models/glowTTS_ASR_ffn_x_vector/200ep/*/no_specaug/ce_ls_0.1/phoneme_pred/train-clean/output/output.hdf\",\n",
    "]\n",
    "lr_files = []\n",
    "for g in globs:\n",
    "    lr_files += glob.glob(g)\n",
    "\n",
    "print(lr_files)\n",
    "exlude = \"speaker_drop\"\n",
    "lr_files = [l for l in lr_files if not exlude in l]\n",
    "# print(lr_files)\n",
    "common_prefix = os.path.commonpath(lr_files)\n",
    "common_sufix = os.path.commonpath([l[::-1] for l in lr_files])[::-1]\n",
    "\n",
    "names = []\n",
    "for f in lr_files:\n",
    "    names.append(f.removeprefix(common_prefix).removesuffix(common_sufix))\n",
    "\n",
    "files = dict(zip(lr_files, names))\n",
    "files, len(lr_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_dict = {\n",
    "#     \"librispeech_glow_asr/pytorch/encoding_test/\": \"\",\n",
    "#     \"tts_architecture/glow_tts/raw_audio/decoder_test/\": \"\",\n",
    "#     \"/\": \" | \",\n",
    "#     \"_encoder_sample_test\": \" Sampled \",\n",
    "#     \"_decoder_test\": \" Decoder output \",\n",
    "#     # \"_encoding_test_blstm\": \"1x512 BLSTM  Mean + log(std)\",\n",
    "#     \"_encoding_test\": \"\",\n",
    "#     \"_ce\": \"Mean + log(std)\",\n",
    "#     \"_mean_only\": \"Mean\",\n",
    "#     \"_encoding_test_mean_only\": \"Mean\",\n",
    "#     \"glowTTS\": \"\",\n",
    "#     \"_maxlike_alignment\": \" MAS \",\n",
    "#     \"_simple_linear\": \" 1x80 linear \",\n",
    "#     \"_multi_layer_ffn\": \" 3x512 linear \",\n",
    "#     \"_blstm\": \" 1x512 BLSTM \",\n",
    "#     \"tts_architecture\": \"\",\n",
    "#     \"enc192\": \"192 channels\",\n",
    "#     \"enc768\": \"768 channels\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting_definition_glow_model = [\"192 channels\", \"768 channels\"]\n",
    "# sorting_definition_model = [\" 1x80 linear \", \" 3x512 linear \", \" 1x512 BLSTM \"]\n",
    "# sorting_definition_mean = [\"mean_only\", \"with_sigma\"]\n",
    "# sorting_definition_input = [\"Mean |\", \"Mean + log(std) |\", \" Sampled  |\", \" Sampled  MAS  |\", \" Decoder output  |\"]\n",
    "# def sorting_function(a):\n",
    "#     if \"mean_only\" in a[1] or \"with_sigma\" in a[1]:\n",
    "#         split_a = a[1].split(\" | \")\n",
    "\n",
    "#         sorting_index_a = sorting_definition_model.index(split_a[0]) * 1e3\n",
    "\n",
    "#         if \"channels\" in a[1]:\n",
    "#             sorting_index_a += sorting_definition_glow_model.index(split_a[1]) * 1e2 + sorting_definition_mean.index(split_a[2]) * 1e1 + sorting_definition_input.index(split_a[3])   \n",
    "#         else:\n",
    "#             sorting_index_a += sorting_definition_mean.index(split_a[1]) * 1e1 + sorting_definition_input.index(split_a[2])\n",
    "\n",
    "#     else:\n",
    "#         split_a = a[1].split(\" | \")\n",
    "\n",
    "#         sorting_index_a = sorting_definition_model.index(split_a[0]) * 1e2\n",
    "#         if \"channels\" in a[1]:\n",
    "#             sorting_index_a += sorting_definition_glow_model.index(split_a[1]) * 1e1 + sorting_definition_input.index(split_a[2])\n",
    "#         else:\n",
    "#             sorting_index_a += sorting_definition_input.index(split_a[1])\n",
    "\n",
    "#     return sorting_index_a\n",
    "\n",
    "# for k,v in new_files.items():\n",
    "#     if \"with_sigma\" in v:\n",
    "#         sorting_function([k,v])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([' 1x80 linear  | 768 channels | mean_only | Mean |', ' 1x80 linear  | 768 channels | mean_only |  Sampled  |', ' 1x80 linear  | 768 channels | mean_only |  Sampled  MAS  |', ' 1x80 linear  | 768 channels | mean_only |  Decoder output  |', ' 1x80 linear  | 768 channels | with_sigma | Mean |', ' 1x80 linear  | 768 channels | with_sigma | Mean + log(std) |', ' 1x80 linear  | 768 channels | with_sigma |  Sampled  |', ' 1x80 linear  | 768 channels | with_sigma |  Sampled  MAS  |', ' 1x80 linear  | 768 channels | with_sigma |  Decoder output  |', ' 3x512 linear  | 768 channels | mean_only | Mean |', ' 3x512 linear  | 768 channels | mean_only |  Sampled  |', ' 3x512 linear  | 768 channels | mean_only |  Sampled  MAS  |', ' 3x512 linear  | 768 channels | mean_only |  Decoder output  |', ' 3x512 linear  | 768 channels | with_sigma | Mean |', ' 3x512 linear  | 768 channels | with_sigma | Mean + log(std) |', ' 3x512 linear  | 768 channels | with_sigma |  Sampled  |', ' 3x512 linear  | 768 channels | with_sigma |  Sampled  MAS  |', ' 3x512 linear  | 768 channels | with_sigma |  Decoder output  |', ' 1x512 BLSTM  | 768 channels | mean_only | Mean |', ' 1x512 BLSTM  | 768 channels | mean_only |  Sampled  |', ' 1x512 BLSTM  | 768 channels | mean_only |  Sampled  MAS  |', ' 1x512 BLSTM  | 768 channels | mean_only |  Decoder output  |', ' 1x512 BLSTM  | 768 channels | with_sigma | Mean |', ' 1x512 BLSTM  | 768 channels | with_sigma | Mean + log(std) |', ' 1x512 BLSTM  | 768 channels | with_sigma |  Sampled  |', ' 1x512 BLSTM  | 768 channels | with_sigma |  Sampled  MAS  |', ' 1x512 BLSTM  | 768 channels | with_sigma |  Decoder output  |'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted_files = dict(sorted(new_files.items(), key=sorting_function))\n",
    "# sorted_files.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Type | Accuary | Cross Entropy |\n",
      "|============|============|============|\n",
      "| /tts_pretrained/no_specaug/asr_target_size/ | 31.04 | 2.872 |\n",
      "| /basic_init/no_specaug/asr_target_size/ | 28.98 | 2.857 |\n",
      "| /basic_init/no_specaug/tts_target_size/ | 27.46 | 2.840 |\n",
      "| /tts_pretrained/no_specaug/ | 31.04 | 2.872 |\n",
      "| /basic_init/no_specaug/ | 28.98 | 2.857 |\n"
     ]
    }
   ],
   "source": [
    "print(\"| Model Type | Accuary | Cross Entropy |\\n|============|============|============|\")\n",
    "\n",
    "for file, name in files.items():\n",
    "    data = h5py.File(file)\n",
    "    epoch_data = get_epoch_data(\n",
    "        file.replace(\"phoneme_pred/train-clean/output/output.hdf\", \"training/work/learning_rates\"), 100\n",
    "    )\n",
    "    if \"dev_loss_ce\" in epoch_data[\"error\"]: \n",
    "        dev_loss_ce = epoch_data[\"error\"][\"dev_loss_ce\"]\n",
    "    else:\n",
    "        dev_loss_ce = np.inf\n",
    "    mean = np.array(data[\"inputs\"]).mean()\n",
    "    print(f\"| {name} | {1e2*mean:.2f} | {dev_loss_ce:.3f} |\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
