#!rnn.py


def dyn_lr_piecewise_linear(
    *, global_train_step: int, learning_rate: float, **_kwargs
) -> float:
    """
    Piecewise linear
    """
    from returnn.config import get_global_config

    config = get_global_config()

    steps = config.int_list("learning_rate_piecewise_steps")
    lrs = config.float_list("learning_rate_piecewise_values")
    assert len(steps) + 1 == len(lrs)

    last_step = 0
    for i, step in enumerate(steps):
        assert step > last_step
        assert global_train_step >= last_step
        if global_train_step < step:
            factor = (global_train_step + 1 - last_step) / (step - last_step)
            return learning_rate * (lrs[i + 1] * factor + lrs[i] * (1 - factor))
        last_step = step

    return learning_rate * lrs[-1]


accum_grad_multiple_step = 4
aux_loss_layers = None
backend = "torch"
batch_size = 2400000
batching = "laplace:.1000"
cleanup_old_models = {"keep": [500], "keep_best_n": 4, "keep_last_n": 1}
debug_print_layer_output_template = True
default_input = "data"
dev = {
    "class": "MetaDataset",
    "data_map": {"data": ("zip_dataset", "data"), "targets": ("align", "data")},
    "datasets": {
        "align": {
            "class": "HDFDataset",
            "files": [
                "/u/schmitt/experiments/segmental_models_2022_23_rf/work/i6_core/returnn/forward/ReturnnForwardJob.xERLI3g7bpFq/output/alignments.hdf"
            ],
            "partition_epoch": 1,
            "seq_list_filter_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/processing/PipelineJob.gTty7UHs0uBu/output/out",
            "use_cache_manager": True,
        },
        "zip_dataset": {
            "audio": {
                "features": "raw",
                "peak_normalization": True,
                "pre_process": None,
                "preemphasis": None,
            },
            "class": "OggZipDataset",
            "epoch_wise_filter": None,
            "fixed_random_subset": None,
            "partition_epoch": 1,
            "path": [
                "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.RvwLniNrgMit/output/out.ogg.zip",
                "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.NSdIHfk1iw2M/output/out.ogg.zip",
            ],
            "segment_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/processing/PipelineJob.gTty7UHs0uBu/output/out",
            "seq_ordering": "sorted_reverse",
            "targets": {
                "bpe_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.codes",
                "class": "BytePairEncoding",
                "seq_postfix": [0],
                "unknown_label": None,
                "vocab_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.vocab",
            },
            "use_cache_manager": True,
        },
    },
    "seq_order_control_dataset": "zip_dataset",
}
device = "gpu"
dynamic_learning_rate = dyn_lr_piecewise_linear
eval_datasets = {
    "devtrain": {
        "class": "MetaDataset",
        "data_map": {"data": ("zip_dataset", "data"), "targets": ("align", "data")},
        "datasets": {
            "align": {
                "class": "HDFDataset",
                "files": [
                    "/u/schmitt/experiments/segmental_models_2022_23_rf/work/i6_core/returnn/forward/ReturnnForwardJob.4k03LS27KUmL/output/alignments.hdf"
                ],
                "partition_epoch": 1,
                "seq_list_filter_file": None,
                "use_cache_manager": True,
            },
            "zip_dataset": {
                "audio": {
                    "features": "raw",
                    "peak_normalization": True,
                    "pre_process": None,
                    "preemphasis": None,
                },
                "class": "OggZipDataset",
                "epoch_wise_filter": None,
                "fixed_random_subset": 3000,
                "partition_epoch": 1,
                "path": [
                    "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.Cbboscd6En6A/output/out.ogg.zip"
                ],
                "segment_file": None,
                "seq_ordering": "sorted_reverse",
                "targets": {
                    "bpe_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.codes",
                    "class": "BytePairEncoding",
                    "seq_postfix": [0],
                    "unknown_label": None,
                    "vocab_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.vocab",
                },
                "use_cache_manager": True,
            },
        },
        "seq_order_control_dataset": "zip_dataset",
    }
}
grad_scaler = None
gradient_clip_global_norm = 5.0
learning_rate = 1.0
learning_rate_file = "learning_rates"
learning_rate_piecewise_steps = [295000, 590000, 652000]
learning_rate_piecewise_values = [1e-05, 0.001, 1e-05, 1e-06]
log = ["./returnn.log"]
log_batch_size = True
log_verbosity = 5
max_seqs = 200
model = "/u/schmitt/experiments/segmental_models_2022_23_rf/work/i6_core/returnn/training/ReturnnTrainingJob.9zPcngFbJCDE/output/models/epoch"
num_epochs = 500
optimizer = {
    "class": "adamw",
    "epsilon": 1e-16,
    "weight_decay": 1e-06,
    "weight_decay_modules_blacklist": [
        "rf.Embedding",
        "rf.LearnedRelativePositionalEncoding",
    ],
}
pos_emb_dropout = 0.1
rf_att_dropout_broadcast = False
save_interval = 1
target = "targets"
task = "train"
torch_dataloader_opts = {"num_workers": 1}
torch_distributed = {}
torch_log_memory_usage = True
train = {
    "class": "MetaDataset",
    "data_map": {"data": ("zip_dataset", "data"), "targets": ("align", "data")},
    "datasets": {
        "align": {
            "class": "HDFDataset",
            "files": [
                "/u/schmitt/experiments/segmental_models_2022_23_rf/work/i6_core/returnn/forward/ReturnnForwardJob.4k03LS27KUmL/output/alignments.hdf"
            ],
            "partition_epoch": 20,
            "seq_list_filter_file": None,
            "use_cache_manager": True,
        },
        "zip_dataset": {
            "audio": {
                "features": "raw",
                "peak_normalization": True,
                "pre_process": None,
                "preemphasis": None,
            },
            "class": "OggZipDataset",
            "epoch_wise_filter": {(1, 5): {"max_mean_len": 1000}},
            "fixed_random_subset": None,
            "partition_epoch": 20,
            "path": [
                "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.Cbboscd6En6A/output/out.ogg.zip"
            ],
            "segment_file": None,
            "seq_ordering": "laplace:.1000",
            "targets": {
                "bpe_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.codes",
                "class": "BytePairEncoding",
                "seq_postfix": [0],
                "unknown_label": None,
                "vocab_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.vocab",
            },
            "use_cache_manager": True,
        },
    },
    "seq_order_control_dataset": "zip_dataset",
}
use_horovod = True
config = {}

locals().update(**config)

import os
import sys

sys.path.insert(0, "/u/schmitt/experiments/segmental_models_2022_23_rf/recipe")
sys.path.insert(1, "/u/schmitt/src/sisyphus")
from returnn.tensor import Dim, batch_dim, single_step_dim
from returnn.tensor.marked_dim import ImplicitDynSizeDim, ImplicitSparseDim

time_dim = Dim(description="time", dimension=None, kind=Dim.Types.Spatial)
audio_dim = Dim(description="audio", dimension=1, kind=Dim.Types.Feature)
out_spatial_dim = Dim(description="out_spatial", dimension=None, kind=Dim.Types.Spatial)
vocab_dim = Dim(description="vocab", dimension=10026, kind=Dim.Types.Spatial)

extern_data = {
    "data": {"dim_tags": [batch_dim, time_dim, audio_dim]},
    "targets": {"dim_tags": [batch_dim, out_spatial_dim], "sparse_dim": vocab_dim},
}

from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.segmental.luca_example_transducer.transducer_model_luca import (
    from_scratch_model_def as _model_def,
)
from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.segmental.luca_example_transducer.transducer_model_luca import (
    _returnn_v2_get_model as get_model,
)
from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.segmental.train import (
    viterbi_training as _train_def,
)
from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.segmental.train import (
    _returnn_v2_train_step as train_step,
)

# https://github.com/rwth-i6/returnn/issues/957
# https://stackoverflow.com/a/16248113/133374
import resource
import sys

try:
    resource.setrlimit(resource.RLIMIT_STACK, (2**29, -1))
except Exception as exc:
    print(f"resource.setrlimit {type(exc).__name__}: {exc}")
sys.setrecursionlimit(10**6)
_cf_cache = {}


def cf(filename):
    "Cache manager"
    from subprocess import check_output, CalledProcessError

    if filename in _cf_cache:
        return _cf_cache[filename]
    if int(os.environ.get("RETURNN_DEBUG", "0")):
        print("use local file: %s" % filename)
        return filename  # for debugging
    try:
        cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    except CalledProcessError:
        print("Cache manager: Error occurred, using local file")
        return filename
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


# -*- mode: python; tab-width: 4 -*-
