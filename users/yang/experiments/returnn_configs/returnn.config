#!rnn.py


accum_grad_multiple_step = 4
allow_random_model_init = True
backend = "torch"
batch_size = 8000000
batching = "random"
conformer_num_layers = 8
debug_print_layer_output_template = True
default_input = "data"
device = "gpu"
forward_data = {
    "buffer_size": 10,
    "class": "MultiProcDataset",
    "dataset": {
        "class": "MetaDataset",
        "data_map": {
            "data": ("zip_dataset", "data"),
            "targets": ("zip_dataset", "classes"),
        },
        "datasets": {
            "zip_dataset": {
                "audio": {
                    "features": "raw",
                    "peak_normalization": True,
                    "pre_process": None,
                    "preemphasis": None,
                },
                "class": "OggZipDataset",
                "epoch_wise_filter": None,
                "fixed_random_subset": None,
                "partition_epoch": 1,
                "path": [
                    "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.RvwLniNrgMit/output/out.ogg.zip",
                    "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.NSdIHfk1iw2M/output/out.ogg.zip",
                ],
#                "segment_file": "/work/asr3/zeyer/schmitt/sisyphus_work_dirs/segmental_models_2022_23_rf/i6_core/returnn/forward/ReturnnForwardJobV2.xwHW3cZnAl6M/work/segment_file",
                "seq_ordering": "sorted_reverse",
                "targets": {
                    "bpe_file": "/work/asr4/zeineldeen/setups-data/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.qhkNn2veTWkV/output/bpe.codes",
                    "class": "BytePairEncoding",
                    "seq_postfix": None,
                    "unknown_label": None,
                    "vocab_file": "/work/asr4/zeineldeen/setups-data/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.qhkNn2veTWkV/output/bpe.vocab",
                },
                "use_cache_manager": True,
            }
        },
        "seq_order_control_dataset": "zip_dataset",
    },
    "num_workers": 4,
}
log = ["./returnn.log"]
log_batch_size = True
log_verbosity = 5
max_seqs = 200
optimizer = {"class": "adamw", "epsilon": 1e-08, "weight_decay": 1e-06}
preload_from_files = {
    "trained_model": {
        "filename": "/work/asr3/zeyer/schmitt/model_checkpoints/segmental_models_2022_23_rf/luca-global-aed-w-ctc-bpe1k-100ep_converted_w_aux_layers.pt",
        "ignore_missing": True,
    }
}
target = "targets"
task = "forward"
tf_log_dir = "returnn-tf-log"
torch_dataloader_opts = {"num_workers": 1}
torch_log_memory_usage = True
config = {}

locals().update(**config)

import os
import sys

sys.path.insert(0, "/u/schmitt/experiments/segmental_models_2022_23_rf/recipe")
sys.path.insert(1, "/u/schmitt/src/sisyphus")
from returnn.tensor import Dim, batch_dim, single_step_dim
from returnn.tensor.marked_dim import ImplicitDynSizeDim, ImplicitSparseDim

time_dim = Dim(description="time", dimension=None, kind=Dim.Types.Spatial)
audio_dim = Dim(description="audio", dimension=1, kind=Dim.Types.Feature)
out_spatial_dim = Dim(description="out_spatial", dimension=None, kind=Dim.Types.Spatial)
non_blank_target_dim = Dim(
    description="non_blank_target_dim", dimension=1056, kind=Dim.Types.Spatial
)

extern_data = {
    "data": {"dim_tags": [batch_dim, time_dim, audio_dim]},
    "targets": {
        "dim_tags": [batch_dim, out_spatial_dim],
        "sparse_dim": non_blank_target_dim,
    },
}

from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.ctc.model import (
    from_scratch_model_def as _model_def,
)
from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.ctc.model import (
    _returnn_v2_get_model as get_model,
)
from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.ctc.realignment import (
    model_realign_ as _realign_def,
)
from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.ctc.realignment import (
    _returnn_v2_forward_step as forward_step,
)
from i6_experiments.users.schmitt.experiments.config.pipelines.global_vs_segmental_2022_23_rf.dependencies.returnn.network_builder_rf.ctc.realignment import (
    _returnn_v2_get_forward_callback as forward_callback,
)

# https://github.com/rwth-i6/returnn/issues/957
# https://stackoverflow.com/a/16248113/133374
import resource
import sys
#import os

#os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

try:
    resource.setrlimit(resource.RLIMIT_STACK, (2**29, -1))
except Exception as exc:
    print(f"resource.setrlimit {type(exc).__name__}: {exc}")
sys.setrecursionlimit(10**6)
_cf_cache = {}


def cf(filename):
    "Cache manager"
    from subprocess import check_output, CalledProcessError

    if filename in _cf_cache:
        return _cf_cache[filename]
    if int(os.environ.get("RETURNN_DEBUG", "0")):
        print("use local file: %s" % filename)
        return filename  # for debugging
    try:
        cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    except CalledProcessError:
        print("Cache manager: Error occurred, using local file")
        return filename
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


# -*- mode: python; tab-width: 4 -*-
